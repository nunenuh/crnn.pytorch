{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import * \n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, ConcatDataset, Subset, DataLoader\n",
    "\n",
    "import torchvision.transforms as VT\n",
    "\n",
    "\n",
    "from ocrnune.data import dataset\n",
    "from ocrnune.models import crnn\n",
    "\n",
    "import ocrnune.transforms as NT\n",
    "from ocrnune.data.dataset import LMDBDataset, BalanceDatasetConcatenator\n",
    "from ocrnune.utils import AttnLabelConverter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 4\n",
    "BATCH_MAX_LENGTH = 25\n",
    "CHARACTER = string.printable[:-6]\n",
    "IMG_SIZE = (32,100)\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.999\n",
    "LRATE = 1.0\n",
    "\n",
    "GRAD_CLIP = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_transform = VT.Compose([\n",
    "    NT.ResizeRatioWithRightPad(size=IMG_SIZE),\n",
    "    VT.ToTensor(),\n",
    "    VT.Normalize(mean=(0.5), std=(0.5))  \n",
    "])\n",
    "\n",
    "val_transform = VT.Compose([\n",
    "    NT.ResizeRatioWithRightPad(size=IMG_SIZE),\n",
    "    VT.ToTensor(),\n",
    "    VT.Normalize(mean=(0.5), std=(0.5))  \n",
    "])\n",
    "\n",
    "\n",
    "trn_path = '/data/lmdb/data_lmdb_release/training'\n",
    "val_path = '/data/lmdb/data_lmdb_release/validation'\n",
    "\n",
    "\n",
    "\n",
    "train_bdc = BalanceDatasetConcatenator(trn_path, dataset_class=LMDBDataset, \n",
    "                                       transform=trn_transform,\n",
    "                                       subdir=('ST', 'MJ'), usage_ratio=(0.5, 0.5),\n",
    "                                       im_size=IMG_SIZE, is_sensitive=True)\n",
    "trainset = train_bdc.get_dataset()\n",
    "\n",
    "\n",
    "valid_bdc = BalanceDatasetConcatenator(val_path, dataset_class=LMDBDataset, \n",
    "                                       transform=val_transform,\n",
    "                                       im_size=IMG_SIZE, is_sensitive=True)\n",
    "validset = valid_bdc.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7221024, 6992)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(validset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocrnune.utils import AttnLabelConverter\n",
    "converter = AttnLabelConverter(CHARACTER)\n",
    "num_class = len(converter.character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "valid_loader = DataLoader(validset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, texts =  next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = AttnLabelConverter(CHARACTER)\n",
    "NUM_CLASS = len(converter.character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = crnn.OCR(num_class=NUM_CLASS, im_size=IMG_SIZE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LRATE, betas=(BETA1, BETA2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.metrics import Accuracy\n",
    "\n",
    "class OCRTaks(pl.LightningModule):\n",
    "    def __init__(self, model, optimizer, criterion, converter, grad_clip=5.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.converter = converter\n",
    "        self.grad_clip = grad_clip\n",
    "    \n",
    "    def forward(self, imgs, text):\n",
    "        output = self.model(imgs, texts)\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def backward(self, trainer, loss, optimizer, optimizer_idx):\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "   \n",
    "    def shared_step(self, batch, batch_idx):\n",
    "        images, texts = batch\n",
    "        texts_encoded, texts_length = self.converter.encode(texts)\n",
    "        \n",
    "        preds = self.model(images, texts_encoded[:, :-1])\n",
    "        targets = texts_encoded[:, 1:]\n",
    "        \n",
    "        loss = self.criterion(preds.view(-1, preds.shape[-1]), targets.contiguous().view(-1))\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch, batch_idx)\n",
    "        result = pl.TrainResult(loss)\n",
    "        result.log_dict({'trn_loss': loss})\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch, batch_idx)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "        result.log_dict({'val_loss': loss})\n",
    "        \n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '../saved_model'\n",
    "# DEFAULTS used by the Trainer\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='checkpoint_on',\n",
    "    mode='min',\n",
    "    prefix='ocr_net_'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_logger = pl_loggers.TensorBoardLogger('../logs/ocr_net')\n",
    "task = OCRTaks(model, optimizer, criterion, converter)\n",
    "trainer = pl.Trainer(gpus=1, logger=tb_logger, checkpoint_callback=checkpoint_callback)\n",
    "trainer.fit(task, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to initialize NVML: Driver/library version mismatch\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
